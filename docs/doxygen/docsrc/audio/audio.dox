/**
\page Audio

\tableofcontents

# ci::audio

These are preliminary docs for cinder's next audio API. Please use this, along with the samples, as a reference to get you started using cinder's newly aded audio tools.

\section features Features

- A flexible and comfortable system for modular audio processing.  We want people to explore what's capable.
- a 'simple api', that lets users easily play audio.
- Wide device and file support
- Built-in support for audio-rate parameter ramping and manipulation.
- a library of DSP tools such as FFT, samplerate conversion, and vector operations.
- Probably goes without saying, but we want this system to be fast.
- For initial release, support for Mac, iOS, Windows 7, 8, and minimal support for XP

\section design Design

The core of the design draws from concepts found in other popular modular audio API's, namely [Web Audio][webaudio] and [Pure Data][puredata], however aims to be closely integrated into cinder's existing design patterns. We also take full advantage of C++11 features such as smart pointers, `std::atomic`'s, and `std::mutex`'s.

A modular api is advantagous because it is proven to be very flexible and allows for reusability without loss in performance.  Still, higher level constructs exist and more will be added as time permits.  The cinder philosophy remains, "easy things easy and hard things possible." 

**iOS Only:**

I've only tested on the device, not simulator.  The iOS simulator has many problems related to audio, limiting its usefulness for testing this code.  Instead, build for mac desktop when testing.

## Viewing the Tests

There are currently only a small handful of samples that are meant to get new users up and running in the samples folder.  To see more extensive usage of the various components, open up one of the test workspaces:

- mac: test/Audio2Test.xcworkspace
- windows: test/Audio2Test.msw/Audio2Test.sln

These are meant to be more for feature and regression testing than anything else, but at the moment they are also the most useful way to see the entire breadth of the available functionality.


\section voice_api Voice API

TODO: move this to after an introduction to Node's, since Voice's are built on top.

The Voice class allows users to easily perform common audio tasks, like playing a sound file. For example:

    SourceFileRef sourceFile = audio::load( loadResource( "soundfile.wav" ) );
    mVoice = audio::Voice::create( sourceFile );
    mVoice->start();


Common tasks like `start()`, `stop()`, and `pause()` are supported. Each Voice has controls for volume and 2d panning.  Here is an example of how you'd control these with the mouse:

    void MyApp::mouseDown( app::MouseEvent event )
    {
    	mVoice->setVolume( 1.0f - (float)event.getPos().y / (float)getWindowHeight() );
    	mVoice->setPan( (float)event.getPos().x / (float)getWindowWidth() );
    }

\subsection voice_nodes How a Voice manages its Nodes

The `Voice::create` method returns a shared_ptr to a Voice sub-class (ex, in the case of playing an audio file, returns `VoiceSamplePlayerRef`, but the user will generally only need to maintain this by storing it in a VoiceRef instance variable. It is necessary to store the returned VoiceRef because once it goes out of scope, it will be disconencted from the audio graph and destroyed.  This is fairly cheap, however (much cheaper than creating the `SourceFile` via `audio::load()`, for example), so if you need to later play a different file, you can safely call `Voice::create()` again and assign it to your `mVoice` instance variable.

The Voice API sits above and ties into the modular API, which is explained below. Each Voice has a virtual `Voice::getInputNode()` member function that returns the `Node` object that is responsible for generating samples. This gives you access to more advanced functionality, like setting extended properties.

Because the Voice internally manages a chain of Node's, the actual `Node` that is connected to the master output can be retrieved with the virtual `Voice::getOutputNode()` member function.  This is currently always the `Pan2d`, however a generic `NodeRef` is returned because the actual type should be opaque to the user.

\subsection voice_direct Manually directing the Voice output

By default, a `Voice` is automatically connected up to `Context::master()`, which represents the active output hardware device (ex. speakers). If you plan to direct the `Voice`'s output to something else, you need to specify this at creation time by passing `false` to `Voice::Options::connectToMaster()`. For example, the following manually connects the `Voice` up to a `Scope` (which allows one to visualize the audio samples, see docs later on), and then to `Context::master()`:

    auto options = audio::Voice::Options().connectToMaster( false );
    mVoice = audio::Voice::create( sourceFile, options );
    mScope = audio::master()->makeNode( new audio::Scope );
  
    mVoice->getOutputNode() >> mScope >> audio::master()->getOutput();

See Also:

- [VoiceBasic](../../samples/_audio/VoiceBasic/src/VoiceBasicApp.cpp)
- [VoiceBasicProcessing](../../samples/_audio/VoiceBasicProcessing/src/VoiceBasicProcessingApp.cpp)

\section modular Modular API

\subsection context Context

The Context class manages platform specific audio processing and thread synchronization between the 'audio' (real-time) and 'user' (typically UI/main, but not limited to) threads. There is one 'master', which is the only hardware-facing Context. All Node's are created using the Context, which is necessary for thread safety:

    auto ctx = audio::Context::master();
    mNode = ctx->makeNode( new NodeType );

\subsection node Node

A Node is the fundamental building block for audio processing graphs. They allow for flexible combinations of synthesis, analysis, effects, file reading/writing, etc., and are meant to be easily subclassed. There are a two important Node types also worth mentioning upfront:

- **OutputNode**: an endpoint at the end of an audio graph. Has no outputs.
- **InputNode**: an endpoint at the beginning of an audio graph. Has no inputs.

Node's are connected together to from an audio graph. For audio to reach the speakers, the last Node in the graph is connected to the Context's OutputNode:

    auto ctx = audio::Context::master();
    mSine = ctx->makeNode( new audio::GenSineNode );
    mGain = ctx->makeNode( new audio::GainNode );
    
    mSine->connect( mGain );
    mGain->connect( ctx->getOutput() );

Node's are connected from source to destination. A convenient shorthand syntax that is meant to represent this is as follows:

    mSine >> mGain >> ctx->getOutput();

To process audio, each Node subclass implements a virtual method `process( Buffer *buffer )`. Processing can be enabled or disabled on a per-Node basis. While `NodeEffect`s are enabled by default, `NodeInput`s must be turned on before they produce any audio. `OutputNode`s are managed by their owning `Context`, which has a similar enabled/disabled syntax:

    mSine->enable();
    ctx->enable();

It is important to note that enabling or disabling the Context effects the processing the entire audio graph - no audio will be processed at all if it is off and 'audio time' will not progress.  Not only can you use this to save on cpu / power when you need to, it is also a useful catch-all way to shut off the audio processing thread.

The reason why the above is true is that, although Node's are (by convention) connected source >> destination, the actual processing follows the 'pull model', i.e. destination (recursively) pulls the source.  The bulk of the work is done by ` Node::pullInputs()`, which ultimately ends up calling the virtual `Node::process()` method with the `Buffer` that should be filled with processed audio.

Other Node features include:

* can be enabled / disabled / connected / disconnected while audio is playing
* supports multiple inputs, which are implicitly summed to their specified number of channels.
* supports multiple outputs, which don't necessarily have to be connected to the Context's output( they will be added to the 'auto pull list').
* Feedback is supported by connecting Node's in a cycle, although for this to make sense there must be a Node that overrides `supportsCycles()` and returns true.  The build in `Delay` is the primary user of this feature.
* If possible (ex. one input, same # channels), a Node will process audio in-place
* Node::ChannelMode allows the channels to be decided based on either a Node's input, it's output, or specified by user.

See Also:

- [NodeBasic](../../samples/_audio/NodeBasic/src/NodeBasicApp.cpp)
- [NodeAdvanced](../../samples/_audio/NodeAdvanced/src/NodeAdvancedApp.cpp)

\subsection device_output Device Output

The default audio Context is the one that sends audio to the default audio hardware device, and by default the number of channels set is two (or one if that is all that is available). This is most commonly accessed with `ci::audio::master()`., which returns the system's default `OutputDeviceNode`. 

###### Specifying a non-default Device

If you need the Context to address a device other than the system default, you must create a LineOut with the appropriate `ci::audio::DeviceRef`, then assign that as the master `Context`'s `OutputNode`:

    ci::audio::DeviceRef device = ci::audio::Device::findDeviceByName( "Device Name" );
    ci::audio::LineOutRef lineOut = ctx->createLineOut( device );
    ctx->setOutput( lineOut );

The device name can be found in your system settings or by iterating the `DeviceRef`'s returned by `Device::getDevices()` and looking at its `getName()` property.  As an alternative to specifying the device by name, you can use `findDeviceByKey()`, which is a platform-agnostic, unique identifier that is internally generated.

###### Specifying a Channel Count Other than Stereo (the default)

If you intend to handle a channel count other than the default stereo pair, you need to create a LineOut and pass in the desired channel count in its optional `Node::Format` argument. 

    auto format = ci::audio::Node::Format().channels( 10 );
    ci::audio::LineOutRef lineOut = ctx->createLineOut( device, format );
    ctx->setOutput( lineOut );

__note__: Replacing the master `Context`'s output will cause a context-wide `Node::uninitialize()` and `Node::initialize()`.  This is because the Context controls variables that the Node's rely on, such as samplerate and frames-per-block.  While in some cases it may be unnoticeable, it's usually a good idea to call `Context::disable()` (or do a more robust halt of your graph) beforehand to prevent unexpected audio clicks.

###### Using ChannelRouterNode to map Node's to specific output channels

While the above explains how to enable output channels greater than the standard stereo pair, addressing individual channels requires a specific Node's for this task, the `ChannelRouterNode`. This Node is used whenever you need to re-map the channels of one Node to another. For example, the following routes a `SamplePlayer` to channel 5 of the Context's output (it has been configured to as a multi-channel output like above):

    auto format = ci::audio::Node::Format().channels( 10 );
    auto channelRouter = ctx->makeNode( new audio::ChannelRouterNode( format ) );
    mSamplePlayer >> mChannelRouter->route( 0, 5 );

The first argument to `ChannelRouterNode::route()` is the input channel index, the second is the output channel index.

If `mSamplePlayer` happens to be stereo, both channels will be mapped, provided that there are enough channels (starting at the ChannelRouterNode's channel index 5 ) to accomodate.  If instead you need to specifically only route a single channel, the route() method can take a third argument to specify the channel count:

    mSamplePlayer >> mChannelRouter->route( 0, 5, 1 );

See Also:

- [MultichannelOutput](../../samples/_audio/MultichannelOutput/src/MultichannelOutputApp.cpp)

\subsection device_input Device Input

FINISH ME

\subsection read_audio Reading and Playing Audio Files

Audio files are represented by the `audio::SourceFile` class, which represents a handle to the file along with all of its properties. Here is how to load a SourceFile from your assets directory:

    mSourceFile = audio::load( loadAsset( "audiofile.wav" ) );


 The main interface for audio file playback is SamplePlayer, which is abstract and comes in two concrete flavors: `BufferPlayerNode`'s hold their audio data in-memory, and `FilePlayerNode`'s stream the audio data from file.

To create and load a `BufferPlayerNode`:

    auto ctx = audio::Context::master();
    mBufferPlayer = ctx->makeNode( new audio::BufferPlayerNode() );
    mBufferPlayer->loadBuffer( mSourceFile );

And to create a `FilePlayerNode`:

    mFilePlayer = ctx->makeNode( new audio::FilePlayerNOde( mSourceFile ) );

Both support reading of file types async; `BufferPlayer::loadBuffer` can be done on a background thread, and FilePlayer can be specified as reading from a background thread during construction. 

Supported File types:

- For mac, see file types [listed here][coreaudio-file-types].
- For windows, see file types [listed here][mediafoundation-file-types]. 
- supported ogg vorbis on all platforms.

See:

\subsection write_audio Writing Audio Files

TODO (not finished implementing)

\subsection monitor Viewing audio using MonitorNode and MonitorSpectralNode

TODO

See Also:

- [NodeAdvanced](../../samples/_audio/NodeAdvanced/src/NodeAdvancedApp.cpp)
- [InputAnalyzer](../../samples/_audio/InputAnalyzer/src/InputAnalyzerApp.cpp)

\section other Other

\subsection third_party Included Third Party Code

There are a few libraries written by third parties, all redistributed in source form and liberally licensed:

- [ooura] general purpose FFT algorithms.
- [r8brain] sample rate converter library, designed by Aleksey Vaneev of Voxengo.
- [oggvorbis] audio decoder / encoder for the ogg file format.

To all of the people responsible for making these available and of such high quality, thank you!


[cinder]: https://github.com/cinder/cinder
[tinderbox]: http://libcinder.org/docs/welcome/TinderBox.html
[dev-forum]: https://forum.libcinder.org/#Forum/developing-cinder
[webaudio]: https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html
[puredata]: http://puredata.info/
[coreaudio-file-types]: https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/CoreAudioOverview/SupportedAudioFormatsMacOSX/SupportedAudioFormatsMacOSX.html
[mediafoundation-file-types]: http://msdn.microsoft.com/en-us/library/windows/desktop/dd757927(v=vs.85).aspx
[ooura]: http://www.kurims.kyoto-u.ac.jp/~ooura/fft.html
[r8brain]: https://code.google.com/p/r8brain-free-src/
[oggvorbis]: http://xiph.org/vorbis/

*/
